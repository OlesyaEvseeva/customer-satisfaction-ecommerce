{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e109ff7",
   "metadata": {},
   "source": [
    "# Data Integration & Preprocessing for Customer Satisfaction Analysis\n",
    "\n",
    "This notebook is part of a larger project exploring customer satisfaction in Brazilian e-commerce using the [Olist dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).  \n",
    "It builds on the cleaned datasets prepared in the previous notebook by joining them into a single analytical table, performing additional data cleaning, and engineering features to support the upcoming analysis.\n",
    "\n",
    "\n",
    "**Goals of this notebook:**\n",
    "\n",
    "- Join the cleaned tables into a consolidated dataset at the order level  \n",
    "- Clean and standardize the merged dataset (e.g., data types, missing values)  \n",
    "- Create new features to capture relevant aspects of orders, products, payments etc.\n",
    "- Export the final dataset for analysis\n",
    "\n",
    "**This notebook is preceded and followed by:**\n",
    "\n",
    "- [Data Cleaning Notebook](./01_data_cleaning.ipynb): loads and prepares the individual raw datasets for integration  \n",
    "- [Exploratory Analysis Notebook](./03_customer-satisfaction-analysis.ipynb): investigates customer satisfaction patterns and key influencing factors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc4607",
   "metadata": {},
   "source": [
    "## **Structure of the Notebook**\n",
    "\n",
    "> _Note: Section links and ‚ÄúBack to top‚Äù links work best in Jupyter environments (e.g., Jupyter Lab or VS Code). They may not work as expected when clicked directly on GitHub._\n",
    "\n",
    "- [Data Integration](#data-integration)\n",
    "  - [Load cleaned datasets](#load-cleaned-csv-files-into-duckdb-tables)\n",
    "  - [Join tables into order-level dataset](#join-tables-into-order-level-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68907a04",
   "metadata": {},
   "source": [
    "## **Data Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98399a",
   "metadata": {},
   "source": [
    "### Load Cleaned CSV Files into DuckDB Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cleaned CSV Files into DuckDB Tables\n",
    "\n",
    "# Path to the folder containing cleaned CSV files\n",
    "data_path = \"../data/cleaned\"\n",
    "\n",
    "# Connect to an in-memory DuckDB database\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "# Load each cleaned dataset into its own DuckDB table\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE customers AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/customers.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE orders AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/orders.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE items AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/items.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE payments AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/payments.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE reviews AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/reviews.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE products AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/products.csv');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE sellers AS \n",
    "    SELECT * FROM read_csv_auto('{data_path}/sellers.csv');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a3807",
   "metadata": {},
   "source": [
    "[ü†â Back to top](#structure-of-the-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d7154",
   "metadata": {},
   "source": [
    "### Join Tables into Order-Level Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69611952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Query to Create Order-Level Analytical Dataset\n",
    "q_orders = \"\"\"\n",
    "WITH item_quantities AS (\n",
    "  -- Aggregate product and seller info per product in each order\n",
    "  SELECT\n",
    "    i.order_id,\n",
    "    i.product_id,\n",
    "    COALESCE(p.product_category_name_english, 'unknown') AS product_category_name,\n",
    "    COALESCE(s.seller_state, 'unknown') AS seller_state,\n",
    "    COALESCE(s.seller_id, 'unknown') AS seller_id,\n",
    "    COUNT(*) AS product_quantity,\n",
    "    SUM(i.price) AS product_price,\n",
    "    SUM(i.freight_value) AS product_freight,\n",
    "    MAX(p.product_weight_g) AS product_weight_g,\n",
    "    MAX(i.shipping_limit_date) AS shipping_limit_date,\n",
    "    AVG(p.product_name_lenght) AS product_name_length,\n",
    "    AVG(p.product_description_lenght) AS product_description_length,\n",
    "    AVG(p.product_photos_qty) AS product_photos_qty,\n",
    "    AVG(p.product_length_cm) AS product_length_cm,\n",
    "    AVG(p.product_height_cm) AS product_height_cm,\n",
    "    AVG(p.product_width_cm) AS product_width_cm\n",
    "  FROM items AS i\n",
    "  LEFT JOIN products AS p ON i.product_id = p.product_id\n",
    "  LEFT JOIN sellers AS s ON i.seller_id = s.seller_id\n",
    "  GROUP BY i.order_id, i.product_id, s.seller_state, s.seller_id, p.product_category_name_english\n",
    "),\n",
    "\n",
    "-- Collapse product category into a single value per order\n",
    "category_per_order AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    CASE \n",
    "      WHEN COUNT(DISTINCT product_category_name) = 1 \n",
    "        THEN MAX(product_category_name)\n",
    "      ELSE 'multiple_categories'\n",
    "    END AS product_category_name\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Collapse seller state into a single value per order\n",
    "seller_state_per_order AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    CASE \n",
    "      WHEN COUNT(DISTINCT seller_state) = 1 \n",
    "        THEN MAX(seller_state)\n",
    "      ELSE 'multiple_states'\n",
    "    END AS seller_state\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Collapse seller ID into a single value per order\n",
    "seller_id_per_order AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    CASE \n",
    "      WHEN COUNT(DISTINCT seller_id) = 1 \n",
    "        THEN MAX(seller_id)\n",
    "      ELSE 'multiple_sellers'\n",
    "    END AS seller_id\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Collapse product ID into a single value per order\n",
    "product_id_per_order AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    CASE \n",
    "      WHEN COUNT(DISTINCT product_id) = 1 \n",
    "        THEN MAX(product_id)\n",
    "      ELSE 'multiple_products'\n",
    "    END AS product_id\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Average product-level features across items in the order\n",
    "product_features_per_order AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    ROUND(AVG(product_name_length)) AS product_name_length,\n",
    "    ROUND(AVG(product_description_length)) AS product_description_length,\n",
    "    ROUND(AVG(product_photos_qty)) AS product_photos_qty,\n",
    "    ROUND(AVG(product_length_cm)) AS product_length_cm,\n",
    "    ROUND(AVG(product_height_cm)) AS product_height_cm,\n",
    "    ROUND(AVG(product_width_cm)) AS product_width_cm\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Aggregate item-level purchase details per order\n",
    "orders_items AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    COUNT(DISTINCT product_id) AS num_unique_products,\n",
    "    ROUND(SUM(product_quantity)) AS num_items,\n",
    "    SUM(product_price) AS total_price,\n",
    "    SUM(product_freight) AS total_freight,\n",
    "    SUM(product_price + product_freight) AS total_amount,\n",
    "    SUM(product_quantity * product_weight_g) AS total_order_weight,\n",
    "    MAX(shipping_limit_date) AS shipping_limit_date\n",
    "  FROM item_quantities\n",
    "  GROUP BY order_id\n",
    "),\n",
    "\n",
    "-- Aggregate payment details per order\n",
    "payments_agg AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    MAX(payment_installments) AS max_payment_installments,\n",
    "    COUNT(payment_sequential) AS n_payment_records, \n",
    "    STRING_AGG(DISTINCT payment_type, ', ' ORDER BY payment_type) AS payment_types,\n",
    "    COUNT(DISTINCT payment_type) AS n_payment_types\n",
    "  FROM payments\n",
    "  GROUP BY order_id\n",
    ")\n",
    "\n",
    "------ Final Joined Order-Level Table ------\n",
    "SELECT\n",
    "  oi.order_id,\n",
    "  r.review_score,\n",
    "  pid.product_id,\n",
    "  cat.product_category_name,\n",
    "  oi.num_unique_products,\n",
    "  oi.num_items,\n",
    "  oi.total_price,\n",
    "  oi.total_freight,\n",
    "  oi.total_amount,\n",
    "  oi.total_order_weight,\n",
    "  ROUND(oi.total_freight / NULLIF(oi.total_amount, 0), 4) AS freight_share,\n",
    "  ROUND(oi.total_freight / NULLIF(oi.total_price, 0), 4) AS freight_to_price_ratio,\n",
    "  pf.product_name_length,\n",
    "  pf.product_description_length,\n",
    "  pf.product_photos_qty,\n",
    "  pf.product_length_cm,\n",
    "  pf.product_height_cm,\n",
    "  pf.product_width_cm,\n",
    "  pa.payment_types,\n",
    "  pa.n_payment_types,\n",
    "  pa.max_payment_installments,\n",
    "  pa.n_payment_records,\n",
    "  oi.shipping_limit_date,\n",
    "  o.order_delivered_carrier_date,\n",
    "\n",
    "  -- Calculate shipping delay (days late shipping vs. promised date)\n",
    "  CASE \n",
    "    WHEN o.order_delivered_carrier_date IS NOT NULL AND oi.shipping_limit_date IS NOT NULL \n",
    "    THEN DATE_PART('day', o.order_delivered_carrier_date - oi.shipping_limit_date)\n",
    "    ELSE NULL \n",
    "  END AS shipping_delay_days,\n",
    "  o.order_delivered_customer_date,\n",
    "  o.order_estimated_delivery_date,\n",
    "\n",
    "  -- Calculate delivery delay (days late vs. estimated)\n",
    "  CASE \n",
    "    WHEN o.order_delivered_customer_date IS NOT NULL AND o.order_estimated_delivery_date IS NOT NULL \n",
    "    THEN DATE_PART('day', o.order_delivered_customer_date - o.order_estimated_delivery_date) \n",
    "    ELSE NULL \n",
    "  END AS delivery_delay_days,\n",
    "  r.review_comment_message,\n",
    "  r.review_creation_date,\n",
    "  r.review_answer_timestamp,\n",
    "\n",
    "  -- Time between review creation and publication (platform-side delay)\n",
    "  CASE \n",
    "    WHEN r.review_answer_timestamp IS NOT NULL AND r.review_creation_date IS NOT NULL \n",
    "    THEN DATE_PART('day', r.review_answer_timestamp - r.review_creation_date) \n",
    "    ELSE NULL \n",
    "  END AS review_processing_delay_days,\n",
    "  \n",
    "  c.customer_unique_id,\n",
    "  c.customer_state,\n",
    "  ssoid.seller_id,\n",
    "  sso.seller_state\n",
    "\n",
    "FROM orders_items AS oi\n",
    "LEFT JOIN orders AS o USING(order_id)\n",
    "LEFT JOIN customers AS c USING(customer_id)\n",
    "LEFT JOIN payments_agg AS pa USING(order_id)\n",
    "LEFT JOIN reviews AS r USING(order_id)\n",
    "LEFT JOIN category_per_order AS cat USING(order_id)\n",
    "LEFT JOIN seller_state_per_order AS sso USING(order_id)\n",
    "LEFT JOIN seller_id_per_order AS ssoid USING(order_id)\n",
    "LEFT JOIN product_features_per_order AS pf USING(order_id)\n",
    "LEFT JOIN product_id_per_order AS pid USING(order_id)\n",
    "\n",
    "-- Filter: only delivered orders with valid review scores\n",
    "WHERE o.order_status = 'delivered' \n",
    "  AND r.review_score IS NOT NULL\n",
    "\n",
    "ORDER BY oi.order_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the final SQL query and convert the result to a pandas DataFrame\n",
    "df_orders = con.execute(q_orders).df()\n",
    "\n",
    "# Show shape of the resulting dataset\n",
    "print(f\"Order-level joined table has {df_orders.shape[0]} rows and {df_orders.shape[1]} columns.\")\n",
    "\n",
    "# Preview the first few rows\n",
    "df_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3fd6b",
   "metadata": {},
   "source": [
    "[ü†â Back to top](#structure-of-the-notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackfuelpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
