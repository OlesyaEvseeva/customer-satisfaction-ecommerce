{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ec260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for better readability in output\n",
    "#pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", '{:,.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7b62b",
   "metadata": {},
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f8f36",
   "metadata": {},
   "source": [
    "### Load and explore all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecc7fe",
   "metadata": {},
   "source": [
    "➤ Load all raw CSV files into individual DataFrames and store them in a dictionary for easier handling and quick access during exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files from the Brazilian E-Commerce dataset into separate DataFrames\n",
    "data_path = \"../data/brazilian/raw/\"\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "customers_raw = pd.read_csv(data_path + \"olist_customers_dataset.csv\")\n",
    "geolocation_raw = pd.read_csv(data_path + \"olist_geolocation_dataset.csv\")\n",
    "orders_raw = pd.read_csv(data_path + \"olist_orders_dataset.csv\")\n",
    "items_raw = pd.read_csv(data_path + \"olist_order_items_dataset.csv\")\n",
    "payments_raw = pd.read_csv(data_path + \"olist_order_payments_dataset.csv\")\n",
    "reviews_raw = pd.read_csv(data_path + \"olist_order_reviews_dataset.csv\")\n",
    "products_raw = pd.read_csv(data_path + \"olist_products_dataset.csv\")\n",
    "sellers_raw = pd.read_csv(data_path + \"olist_sellers_dataset.csv\")\n",
    "translation_raw = pd.read_csv(data_path + \"product_category_name_translation.csv\")\n",
    "\n",
    "# Store all DataFrames in a dictionary for easier looping and inspection\n",
    "dataframes_raw = {\n",
    "    \"customers\": customers_raw,\n",
    "    \"geolocation\": geolocation_raw,\n",
    "    \"orders\": orders_raw,\n",
    "    \"items\": items_raw,\n",
    "    \"payments\": payments_raw,\n",
    "    \"reviews\": reviews_raw,\n",
    "    \"products\": products_raw,\n",
    "    \"sellers\": sellers_raw,\n",
    "    \"translation\": translation_raw,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf65185",
   "metadata": {},
   "source": [
    "The following tables are included in the Brazilian E-Commerce dataset:\n",
    "\n",
    "- `customers`: customer information  \n",
    "- `geolocation`: geographical coordinates by zip code prefix  \n",
    "- `orders`: order details including status and timestamps  \n",
    "- `items`: product-level details for each order  \n",
    "- `payments`: payment methods, amounts and installment information \n",
    "- `reviews`: customer reviews and ratings  \n",
    "- `products`: product attributes including category and dimensions\n",
    "- `sellers`: seller information  \n",
    "- `translation`: Portuguese-to-English product category mapping \n",
    "\n",
    "*Note: Original file names such as `olist_customers_dataset.csv` were renamed to simpler identifiers like `customers` for ease of use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab35b00",
   "metadata": {},
   "source": [
    "➤  Summary of all tables using `.shape`, column names, and duplicate counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20441cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to summarize a dictionary of DataFrames\n",
    "def summarize_dataframes(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of DataFrames and returns a summary DataFrame\n",
    "        with the following information for each:\n",
    "        - name: the key name from the dictionary\n",
    "        - rows: number of rows in the DataFrame\n",
    "        - columns: number of columns\n",
    "        - column_names: a list of column names\n",
    "        - duplicates: number of duplicated rows\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Loop over each DataFrame in the dictionary\n",
    "    for name, df in df_dict.items():\n",
    "        summary.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"rows\": df.shape[0],\n",
    "                \"columns\": df.shape[1],\n",
    "                \"column_names\": list(df.columns),\n",
    "                \"duplicates\": df.duplicated().sum(),\n",
    "            }\n",
    "        )\n",
    "    # Return a summary DataFrame\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "# Call the function and display the summary of all loaded DataFrames\n",
    "summarize_dataframes(dataframes_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59cac0",
   "metadata": {},
   "source": [
    "➤  Quick sampling of 5 rows from each table for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 5 rows from each DataFrame for a quick visual inspection\n",
    "for name, df in dataframes_raw.items():\n",
    "    print(f'{name.capitalize()}:')\n",
    "    display(df.sample(5))\n",
    "    print(\"-\"*130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3a074",
   "metadata": {},
   "source": [
    "➤ Column-wise overview including dtypes, missing values, and unique counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of column properties (dtypes, missing values, uniques) for all DataFrames\n",
    "def overview(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Creates and displays a column-wise overview for each DataFrame in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        df_dict (dict): A dictionary of DataFrames (e.g., {'orders': orders, ...})\n",
    "\n",
    "    Displays:\n",
    "        For each DataFrame:\n",
    "            - Data type\n",
    "            - Non-null count\n",
    "            - Missing value count and percentage\n",
    "            - Missing value percentage\n",
    "            - Number of unique values\n",
    "            - Unique values\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        print(f'{name.capitalize()}:')\n",
    "        summary = pd.DataFrame(\n",
    "                {\n",
    "                    \"dtype\": df.dtypes,\n",
    "                    \"total\": df.count(),\n",
    "                    \"missing_n\": df.isna().sum(),\n",
    "                    \"missing_%\": df.isna().mean() * 100,\n",
    "                    \"uniques_n\": df.nunique(),\n",
    "                    \"uniques\": [df[col].unique() for col in df.columns],\n",
    "                }\n",
    "        )\n",
    "        display(summary)   \n",
    "        print(\"-\"*130)\n",
    "\n",
    "overview(dataframes_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f5dbc",
   "metadata": {},
   "source": [
    "➤ Quick statistical overview of all numeric columns in each raw table to spot any unusual values or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48afe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize basic statistics of all numeric columns for each DataFrame in the dictionary\n",
    "def describe_numeric_columns(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Displays a transposed summary of descriptive statistics (.describe().T)\n",
    "    for all numeric columns in each DataFrame within the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    df_dict (dict): A dictionary where keys are table names and values are pandas DataFrames.\n",
    "\n",
    "    Notes:\n",
    "    - If a DataFrame has no numeric columns, a message is printed instead.\n",
    "    - The output includes a visual summary using display() for easier inspection in notebooks.\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        print(f\"{name.capitalize()}:\")\n",
    "        numeric_df = df.select_dtypes(include=\"number\")\n",
    "\n",
    "        if numeric_df.empty:\n",
    "            print(\"No numeric columns to describe.\")\n",
    "        else:\n",
    "            display(numeric_df.describe().T)\n",
    "\n",
    "        print(\"-\" * 130)\n",
    "\n",
    "\n",
    "describe_numeric_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d9ce0",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7241a32",
   "metadata": {},
   "source": [
    "➤ Copy raw DataFrames into a new working dictionary to preserve the original data before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with copies of all raw DataFrames\n",
    "def copy_raw_dataframes(raw_dict, exclude=None):\n",
    "    \"\"\"\n",
    "    Creates copies of raw DataFrames to preserve the original data before any cleaning steps.\n",
    "\n",
    "    Parameters:\n",
    "        raw_dict (dict): Dictionary containing raw DataFrames.\n",
    "        exclude (list): List of table names to exclude from copying.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with copies of the DataFrames.\n",
    "    \"\"\"\n",
    "    exclude = exclude or []\n",
    "    copy_dict = {}\n",
    "\n",
    "    for name, df in raw_dict.items():\n",
    "        if name in exclude:\n",
    "            continue\n",
    "        copy_dict[name] = df.copy()\n",
    "\n",
    "    return copy_dict\n",
    "\n",
    "\n",
    "dataframes = copy_raw_dataframes(dataframes_raw, exclude=[\"geolocation\"])  # exclude 'geolocation', which is not used in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c6183",
   "metadata": {},
   "source": [
    "### Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d59d0",
   "metadata": {},
   "source": [
    "➤ Missing product-related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the 'products' DataFrame from the dictionary for further processing\n",
    "products = dataframes[\"products\"].copy()\n",
    "\n",
    "# Display the number of missing values in each column of the 'products' DataFrame\n",
    "products.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where any important product-related column is missing\n",
    "products_missing_cols = [\n",
    "    \"product_category_name\",\n",
    "    \"product_name_lenght\",\n",
    "    \"product_description_lenght\",\n",
    "    \"product_photos_qty\"\n",
    "]\n",
    "\n",
    "number_missing_all = products[products_missing_cols].isna().any(axis=1).sum()\n",
    "\n",
    "# Print the number and percentage of affected rows\n",
    "print(\n",
    "    f\"{number_missing_all} rows have missing values in key product-related columns \"\n",
    "    f\"({number_missing_all / products.shape[0]:.1%} of the products table).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where all key product-related columns are missing (category, name length, description length, and photo count)\n",
    "products.dropna(subset=products_missing_cols, how=\"all\", inplace=True)\n",
    "\n",
    "# Display the number of missing values remaining in each column\n",
    "products.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcaeb2",
   "metadata": {},
   "source": [
    "Rows missing `product_category_name` (610 in total) are dropped, as this column is critical for analyzing product-level trends in customer satisfaction. Without it, meaningful grouping and interpretation are not possible.\n",
    "\n",
    "These rows also lack values in other key descriptive columns — such as `product_name_lenght`, `product_description_lenght`, and `product_photos_qty` — which are relevant for understanding how product presentation may affect customer perception. Since these fields are simultaneously missing, and no reliable imputation method is available, we exclude these rows entirely.\n",
    "\n",
    "Other missing fields, such as product dimensions and weight, are not directly relevant to our current analysis. However, to ensure the dataset remains reusable for future projects, we will impute the missing values for `product_weight_g`, `product_length_cm`, `product_height_cm`, and `product_width_cm` using median values from products in the same category and a similar price range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that describe the product's physical dimensions\n",
    "size_cols = [\n",
    "    \"product_weight_g\",\n",
    "    \"product_length_cm\",\n",
    "    \"product_height_cm\",\n",
    "    \"product_width_cm\",\n",
    "]\n",
    "\n",
    "# View rows where at least one of the size columns has missing values\n",
    "products[products[size_cols].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ebb3d",
   "metadata": {},
   "source": [
    "There is now only one row in the dataset with missing values (`product_id` equal to `'09ff539a621711667c43eba6a3bd8466'`), and it happens to be missing all four size-related columns. This product belongs to the `'bebe'` category. Despite this being an isolated case, we will write the following code in a generalized way to ensure it can be reused for other projects or datasets with more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge median item price into the products table to enable price-based imputation\n",
    "items = dataframes[\"items\"].copy()\n",
    "price_by_product = items.groupby(\"product_id\")[\"price\"].median().reset_index()\n",
    "products_price = products.merge(price_by_product, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392411ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price bins (quartiles) within each product category, based on median price\n",
    "# This helps identify similar products by both category and price level\n",
    "products_price[\"price_bin\"] = products_price.groupby(\"product_category_name\")[\n",
    "    \"price\"\n",
    "].transform(lambda x: pd.qcut(x, q=4, duplicates=\"drop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3687971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing size values using the median for each (category, price_bin) group\n",
    "for col in size_cols:\n",
    "    # Calculate the group-specific median for the current column\n",
    "    group_medians = products_price.groupby([\"product_category_name\", \"price_bin\"])[\n",
    "        col\n",
    "    ].transform(\"median\")\n",
    "    # Fill missing values in the current column using the group medians\n",
    "    products_price[col] = products_price[col].fillna(group_medians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of missing values remaining in each column for verification\n",
    "# Note: 'price_bin' may have missing values if a product belongs to a category with only one product,\n",
    "# since quartile-based binning (qcut) cannot be applied in such cases.\n",
    "print(products_price.isna().sum())\n",
    "# Verify that the one product with missing values was successfully imputed\n",
    "products_price[products_price[\"product_id\"] == '09ff539a621711667c43eba6a3bd8466']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d892878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the helper columns and save the cleaned/imputed data back to 'products'\n",
    "products = products_price.drop(columns=[\"price\", \"price_bin\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackfuelpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
