{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ec260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for better readability in output\n",
    "#pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", '{:,.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7b62b",
   "metadata": {},
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f8f36",
   "metadata": {},
   "source": [
    "### Load and explore all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecc7fe",
   "metadata": {},
   "source": [
    "➤ Load all raw CSV files into individual DataFrames and store them in a dictionary for easier handling and quick access during exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files from the Brazilian E-Commerce dataset into separate DataFrames\n",
    "data_path = \"../data/brazilian/raw/\"\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "customers_raw = pd.read_csv(data_path + \"olist_customers_dataset.csv\")\n",
    "geolocation_raw = pd.read_csv(data_path + \"olist_geolocation_dataset.csv\")\n",
    "orders_raw = pd.read_csv(data_path + \"olist_orders_dataset.csv\")\n",
    "items_raw = pd.read_csv(data_path + \"olist_order_items_dataset.csv\")\n",
    "payments_raw = pd.read_csv(data_path + \"olist_order_payments_dataset.csv\")\n",
    "reviews_raw = pd.read_csv(data_path + \"olist_order_reviews_dataset.csv\")\n",
    "products_raw = pd.read_csv(data_path + \"olist_products_dataset.csv\")\n",
    "sellers_raw = pd.read_csv(data_path + \"olist_sellers_dataset.csv\")\n",
    "translation_raw = pd.read_csv(data_path + \"product_category_name_translation.csv\")\n",
    "\n",
    "# Store all DataFrames in a dictionary for easier looping and inspection\n",
    "dataframes_raw = {\n",
    "    \"customers\": customers_raw,\n",
    "    \"geolocation\": geolocation_raw,\n",
    "    \"orders\": orders_raw,\n",
    "    \"items\": items_raw,\n",
    "    \"payments\": payments_raw,\n",
    "    \"reviews\": reviews_raw,\n",
    "    \"products\": products_raw,\n",
    "    \"sellers\": sellers_raw,\n",
    "    \"translation\": translation_raw,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf65185",
   "metadata": {},
   "source": [
    "The following tables are included in the Brazilian E-Commerce dataset:\n",
    "\n",
    "- `customers`: customer information  \n",
    "- `geolocation`: geographical coordinates by zip code prefix  \n",
    "- `orders`: order details including status and timestamps  \n",
    "- `items`: product-level details for each order  \n",
    "- `payments`: payment methods, amounts and installment information \n",
    "- `reviews`: customer reviews and ratings  \n",
    "- `products`: product attributes including category and dimensions\n",
    "- `sellers`: seller information  \n",
    "- `translation`: Portuguese-to-English product category mapping \n",
    "\n",
    "*Note: Original file names such as `olist_customers_dataset.csv` were renamed to simpler identifiers like `customers` for ease of use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab35b00",
   "metadata": {},
   "source": [
    "➤  Summary of all tables using `.shape`, column names, and duplicate counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20441cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to summarize a dictionary of DataFrames\n",
    "def summarize_dataframes(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of DataFrames and returns a summary DataFrame\n",
    "        with the following information for each:\n",
    "        - name: the key name from the dictionary\n",
    "        - rows: number of rows in the DataFrame\n",
    "        - columns: number of columns\n",
    "        - column_names: a list of column names\n",
    "        - duplicates: number of duplicated rows\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Loop over each DataFrame in the dictionary\n",
    "    for name, df in df_dict.items():\n",
    "        summary.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"rows\": df.shape[0],\n",
    "                \"columns\": df.shape[1],\n",
    "                \"column_names\": list(df.columns),\n",
    "                \"duplicates\": df.duplicated().sum(),\n",
    "            }\n",
    "        )\n",
    "    # Return a summary DataFrame\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "# Call the function and display the summary of all loaded DataFrames\n",
    "summarize_dataframes(dataframes_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59cac0",
   "metadata": {},
   "source": [
    "➤  Quick sampling of 5 rows from each table for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 5 rows from each DataFrame for a quick visual inspection\n",
    "for name, df in dataframes_raw.items():\n",
    "    print(f'{name.capitalize()}:')\n",
    "    display(df.sample(5))\n",
    "    print(\"-\"*130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3a074",
   "metadata": {},
   "source": [
    "➤ Column-wise overview including dtypes, missing values, and unique counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of column properties (dtypes, missing values, uniques) for all DataFrames\n",
    "def overview(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Creates and displays a column-wise overview for each DataFrame in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        df_dict (dict): A dictionary of DataFrames (e.g., {'orders': orders, ...})\n",
    "\n",
    "    Displays:\n",
    "        For each DataFrame:\n",
    "            - Data type\n",
    "            - Non-null count\n",
    "            - Missing value count and percentage\n",
    "            - Missing value percentage\n",
    "            - Number of unique values\n",
    "            - Unique values\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        print(f'{name.capitalize()}:')\n",
    "        summary = pd.DataFrame(\n",
    "                {\n",
    "                    \"dtype\": df.dtypes,\n",
    "                    \"total\": df.count(),\n",
    "                    \"missing_n\": df.isna().sum(),\n",
    "                    \"missing_%\": df.isna().mean() * 100,\n",
    "                    \"uniques_n\": df.nunique(),\n",
    "                    \"uniques\": [df[col].unique() for col in df.columns],\n",
    "                }\n",
    "        )\n",
    "        display(summary)   \n",
    "        print(\"-\"*130)\n",
    "\n",
    "overview(dataframes_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f5dbc",
   "metadata": {},
   "source": [
    "➤ Quick statistical overview of all numeric columns in each raw table to spot any unusual values or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48afe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize basic statistics of all numeric columns for each DataFrame in the dictionary\n",
    "def describe_numeric_columns(df_dict=dataframes_raw):\n",
    "    \"\"\"\n",
    "    Displays a transposed summary of descriptive statistics (.describe().T)\n",
    "    for all numeric columns in each DataFrame within the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    df_dict (dict): A dictionary where keys are table names and values are pandas DataFrames.\n",
    "\n",
    "    Notes:\n",
    "    - If a DataFrame has no numeric columns, a message is printed instead.\n",
    "    - The output includes a visual summary using display() for easier inspection in notebooks.\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        print(f\"{name.capitalize()}:\")\n",
    "        numeric_df = df.select_dtypes(include=\"number\")\n",
    "\n",
    "        if numeric_df.empty:\n",
    "            print(\"No numeric columns to describe.\")\n",
    "        else:\n",
    "            display(numeric_df.describe().T)\n",
    "\n",
    "        print(\"-\" * 130)\n",
    "\n",
    "\n",
    "describe_numeric_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d9ce0",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7241a32",
   "metadata": {},
   "source": [
    "➤ Copy raw DataFrames into a new working dictionary to preserve the original data before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with copies of all raw DataFrames\n",
    "def copy_raw_dataframes(raw_dict, exclude=None):\n",
    "    \"\"\"\n",
    "    Creates copies of raw DataFrames to preserve the original data before any cleaning steps.\n",
    "\n",
    "    Parameters:\n",
    "        raw_dict (dict): Dictionary containing raw DataFrames.\n",
    "        exclude (list): List of table names to exclude from copying.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with copies of the DataFrames.\n",
    "    \"\"\"\n",
    "    exclude = exclude or []\n",
    "    copy_dict = {}\n",
    "\n",
    "    for name, df in raw_dict.items():\n",
    "        if name in exclude:\n",
    "            continue\n",
    "        copy_dict[name] = df.copy()\n",
    "\n",
    "    return copy_dict\n",
    "\n",
    "\n",
    "dataframes = copy_raw_dataframes(dataframes_raw, exclude=[\"geolocation\"])  # exclude 'geolocation', which is not used in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c6183",
   "metadata": {},
   "source": [
    "### Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d59d0",
   "metadata": {},
   "source": [
    "➤ Missing product-related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the 'products' DataFrame from the dictionary for further processing\n",
    "products = dataframes[\"products\"].copy()\n",
    "\n",
    "# Display the number of missing values in each column of the 'products' DataFrame\n",
    "products.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where any important product-related column is missing\n",
    "products_missing_cols = [\n",
    "    \"product_category_name\",\n",
    "    \"product_name_lenght\",\n",
    "    \"product_description_lenght\",\n",
    "    \"product_photos_qty\"\n",
    "]\n",
    "\n",
    "number_missing_all = products[products_missing_cols].isna().any(axis=1).sum()\n",
    "\n",
    "# Print the number and percentage of affected rows\n",
    "print(\n",
    "    f\"{number_missing_all} rows have missing values in key product-related columns \"\n",
    "    f\"({number_missing_all / products.shape[0]:.1%} of the products table).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where all key product-related columns are missing (category, name length, description length, and photo count)\n",
    "products.dropna(subset=products_missing_cols, how=\"all\", inplace=True)\n",
    "\n",
    "# Display the number of missing values remaining in each column\n",
    "products.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcaeb2",
   "metadata": {},
   "source": [
    "Rows missing `product_category_name` (610 in total) are dropped, as this column is critical for analyzing product-level trends in customer satisfaction. Without it, meaningful grouping and interpretation are not possible.\n",
    "\n",
    "These rows also lack values in other key descriptive columns — such as `product_name_lenght`, `product_description_lenght`, and `product_photos_qty` — which are relevant for understanding how product presentation may affect customer perception. Since these fields are simultaneously missing, and no reliable imputation method is available, we exclude these rows entirely.\n",
    "\n",
    "Other missing fields, such as product dimensions and weight, are not directly relevant to our current analysis. However, to ensure the dataset remains reusable for future projects, we will impute the missing values for `product_weight_g`, `product_length_cm`, `product_height_cm`, and `product_width_cm` using median values from products in the same category and a similar price range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that describe the product's physical dimensions\n",
    "size_cols = [\n",
    "    \"product_weight_g\",\n",
    "    \"product_length_cm\",\n",
    "    \"product_height_cm\",\n",
    "    \"product_width_cm\",\n",
    "]\n",
    "\n",
    "# View rows where at least one of the size columns has missing values\n",
    "products[products[size_cols].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ebb3d",
   "metadata": {},
   "source": [
    "There is now only one row in the dataset with missing values (`product_id` equal to `'09ff539a621711667c43eba6a3bd8466'`), and it happens to be missing all four size-related columns. This product belongs to the `'bebe'` category. Despite this being an isolated case, we will write the following code in a generalized way to ensure it can be reused for other projects or datasets with more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge median item price into the products table to enable price-based imputation\n",
    "items = dataframes[\"items\"].copy()\n",
    "price_by_product = items.groupby(\"product_id\")[\"price\"].median().reset_index()\n",
    "products_price = products.merge(price_by_product, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392411ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price bins (quartiles) within each product category, based on median price\n",
    "# This helps identify similar products by both category and price level\n",
    "products_price[\"price_bin\"] = products_price.groupby(\"product_category_name\")[\n",
    "    \"price\"\n",
    "].transform(lambda x: pd.qcut(x, q=4, duplicates=\"drop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3687971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing size values using the median for each (category, price_bin) group\n",
    "for col in size_cols:\n",
    "    # Calculate the group-specific median for the current column\n",
    "    group_medians = products_price.groupby([\"product_category_name\", \"price_bin\"])[\n",
    "        col\n",
    "    ].transform(\"median\")\n",
    "    # Fill missing values in the current column using the group medians\n",
    "    products_price[col] = products_price[col].fillna(group_medians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of missing values remaining in each column for verification\n",
    "# Note: 'price_bin' may have missing values if a product belongs to a category with only one product,\n",
    "# since quartile-based binning (qcut) cannot be applied in such cases.\n",
    "print(products_price.isna().sum())\n",
    "# Verify that the one product with missing values was successfully imputed\n",
    "products_price[products_price[\"product_id\"] == '09ff539a621711667c43eba6a3bd8466']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d892878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the helper columns and save the cleaned/imputed data back to 'products'\n",
    "products = products_price.drop(columns=[\"price\", \"price_bin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9f4cc",
   "metadata": {},
   "source": [
    "➤ Missing English translation for product category names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667d24f",
   "metadata": {},
   "source": [
    "Another issue identified during the data overview stage is that the `products` table contains two category names that are missing from the `translation` table. To ensure consistency in our analysis, we will merge the two tables to add English category names and manually fill in the missing translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge products with the translation table\n",
    "translation = dataframes[\"translation\"].copy()\n",
    "\n",
    "products_translated = pd.merge(products, translation, how=\"left\", on=\"product_category_name\")\n",
    "\n",
    "# Identify product categories that are missing English translations\n",
    "categories_missing_translation = products_translated[\n",
    "    products_translated[\"product_category_name_english\"].isna()\n",
    "][\"product_category_name\"].unique()\n",
    "\n",
    "print(f\"Categories missing English translation: {categories_missing_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358009bc",
   "metadata": {},
   "source": [
    "The category name `'pc_gamer'` can remain unchanged.\n",
    "`'portateis_cozinha_e_preparadores_de_alimentos'` refers to kitchen appliances and food preparation tools and will be translated as `'kitchen_appliances_and_food_processors'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual translation for missing categories\n",
    "category_translation_dict = {\n",
    "    \"pc_gamer\": \"pc_gamer\",\n",
    "    \"portateis_cozinha_e_preparadores_de_alimentos\": \"kitchen_appliances_and_food_processors\",\n",
    "}\n",
    "\n",
    "# Fill in missing translations using the manual dictionary\n",
    "products_translated[\"product_category_name_english\"] = products_translated[\n",
    "    \"product_category_name_english\"\n",
    "].fillna(products_translated[\"product_category_name\"].map(category_translation_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d84f8",
   "metadata": {},
   "source": [
    "Now that we have handled the missing values in the `products` table and added English translations for all product categories, we can update the table in the dictionary. The `translation` table is no longer needed and will be removed.  \n",
    "Columns related to product dimensions, weight, and the original Portuguese category names will be dropped at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'products' table in the dictionary with the translated version\n",
    "dataframes[\"products\"] = products_translated\n",
    "\n",
    "# Remove the 'translation' table from the dictionary since it's no longer needed\n",
    "del dataframes[\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e3550",
   "metadata": {},
   "source": [
    "### Handling duplicates in `reviews` table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b81d5",
   "metadata": {},
   "source": [
    "During the data overview stage, we found that neither `review_ids` nor `order_ids` are unique in the `reviews` table. However, for a consistent and reliable analysis of customer satisfaction, it is essential to ensure a one-to-one relationship between reviews and orders — meaning each `order_id` should be associated with exactly one `review_id`, and each `review_id` should correspond to only one `order_id`.\n",
    "This prevents double-counting reviews and ensures that satisfaction scores are accurately linked to individual orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06a152",
   "metadata": {},
   "source": [
    "➤ Handling duplicates in `reviews_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fef442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the reviews and items tables\n",
    "reviews = dataframes[\"reviews\"].copy()\n",
    "items = dataframes[\"items\"].copy()\n",
    "\n",
    "# Filter rows where the same review_id appears more than once\n",
    "duplicate_reviews = reviews[reviews.duplicated(\"review_id\", keep=False)].copy()\n",
    "\n",
    "# Count how many times each review_id appears\n",
    "review_id_counts = duplicate_reviews[\"review_id\"].value_counts()\n",
    "\n",
    "# Add the count back to the duplicate_reviews dataframe\n",
    "duplicate_reviews[\"review_id_count\"] = duplicate_reviews[\"review_id\"].map(review_id_counts)\n",
    "\n",
    "# Merge with items table to inspect what products are tied to these duplicated reviews\n",
    "duplicate_reviews_items = pd.merge(duplicate_reviews, items, how=\"left\", on=\"order_id\")\n",
    "\n",
    "# Report the number of duplicated review_id entries\n",
    "print(\n",
    "    f\"Number of duplicated review_id entries: {duplicate_reviews.shape[0]} out of {reviews.shape[0]} \"\n",
    "    f\"({duplicate_reviews.shape[0] / reviews.shape[0]:.1%} of the reviews dataset).\"\n",
    ")\n",
    "\n",
    "# Sort by frequency of review_id, then review_id and order_id for easier inspection\n",
    "duplicate_reviews_items_sorted = duplicate_reviews_items.sort_values(\n",
    "    [\"review_id_count\", \"review_id\", \"order_id\"], ascending=[False, True, True]\n",
    ")\n",
    "\n",
    "# Display top rows with the most duplicated review_ids\n",
    "duplicate_reviews_items_sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4386d",
   "metadata": {},
   "source": [
    "The table above shows examples of `review_id`s that appear multiple times, each linked to a different `order_id`. These cases likely involve multi-item purchases — either of the same or different products — that should have been recorded under a single `order_id`, but were incorrectly split while sharing the same review.\n",
    "\n",
    "Since this duplication affects only 1.6% of the reviews and introduces ambiguity into the mapping between `orders` and `reviews`, we consider it safer to remove these inconsistent entries from the dataset to preserve the integrity of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78356652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify review_ids that appear more than once\n",
    "duplicated_ids = reviews[\"review_id\"][reviews[\"review_id\"].duplicated(keep=False)]\n",
    "\n",
    "# Remove rows with duplicated review_ids\n",
    "reviews_no_duplicated_review_id = reviews[~reviews[\"review_id\"].isin(duplicated_ids)]\n",
    "\n",
    "# Print summary\n",
    "print(\n",
    "    f\"Number of rows in updated reviews table: {reviews_no_duplicated_review_id.shape[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of unique review_ids: {reviews_no_duplicated_review_id['review_id'].nunique()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf31ff0",
   "metadata": {},
   "source": [
    "➤ Handling duplicates in `order_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same order_id appears more than once\n",
    "duplicate_orders = reviews_no_duplicated_review_id[\n",
    "    reviews_no_duplicated_review_id.duplicated(\"order_id\", keep=False)\n",
    "].copy()\n",
    "\n",
    "# Count how many times each order_id appears\n",
    "order_id_counts = duplicate_orders[\"order_id\"].value_counts()\n",
    "\n",
    "# Add the count as a new column for easier inspection\n",
    "duplicate_orders[\"order_id_count\"] = duplicate_orders[\"order_id\"].map(order_id_counts)\n",
    "\n",
    "# Merge with items table to inspect which products are tied to these duplicated orders\n",
    "duplicate_orders_items = pd.merge(duplicate_orders, items, how=\"left\", on=\"order_id\")\n",
    "\n",
    "# Report the number of duplicated order_id entries\n",
    "print(\n",
    "    f\"Number of duplicated order_id entries: {duplicate_orders.shape[0]} out of {reviews_no_duplicated_review_id.shape[0]} \"\n",
    "    f\"({duplicate_orders.shape[0] / reviews_no_duplicated_review_id.shape[0]:.1%} of the reviews dataset after removing review_id duplicates).\"\n",
    ")\n",
    "\n",
    "# Sort by frequency of order_id, then by order_id and review_id\n",
    "duplicate_orders_items_sorted = duplicate_orders_items.sort_values(\n",
    "    [\"order_id_count\", \"order_id\", \"review_id\"], ascending=[False, True, True]\n",
    ")\n",
    "\n",
    "# Display top rows with the most duplicated order_ids\n",
    "duplicate_orders_items_sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978061fb",
   "metadata": {},
   "source": [
    "In many cases, the same `order_id` is associated with multiple `review_id`s, which might differ in content or score. These appear to be edited versions of the same customer review, mistakenly recorded as separate entries.\n",
    "To ensure that each order is represented by only one review, we keep only the latest available review per `order_id`, based on the `review_answer_timestamp` field, which offers the most precise timestamp available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert review_answer_timestamp to datetime\n",
    "reviews_no_duplicated_review_id.loc[:, \"review_answer_timestamp\"] = pd.to_datetime(\n",
    "    reviews_no_duplicated_review_id[\"review_answer_timestamp\"], errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24269cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and keep the latest review per order_id\n",
    "reviews_cleaned = reviews_no_duplicated_review_id.sort_values(\n",
    "    \"review_answer_timestamp\"\n",
    ").drop_duplicates(subset=\"order_id\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441338b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of rows and number of unique review_ids and order_ids in cleaned table\n",
    "print(f\"Number of rows in cleaned reviews table: {reviews_cleaned.shape[0]}\")\n",
    "print(\"Number of unique review_ids and order_ids:\")\n",
    "print(reviews_cleaned[['review_id', \"order_id\"]].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed0407",
   "metadata": {},
   "source": [
    "Now that both `review_id` and `order_id` columns are unique, we can update the `dataframes` dictionary with the `reviews_cleaned` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ac673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'reviews' table in the dictionary with the cleaned version\n",
    "dataframes[\"reviews\"] = reviews_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a149c",
   "metadata": {},
   "source": [
    "### Saving cleaned dataframes as `csv.` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframes as .csv files\n",
    "save_path = (\n",
    "    \"c:\\\\Users\\\\Olesya\\\\Документы\\\\Projects\\\\stackfuelpp\\\\data\\\\brazilian\\\\cleaned\"\n",
    ")\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    file_path = os.path.join(save_path, f\"{name}.csv\")\n",
    "    df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackfuelpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
